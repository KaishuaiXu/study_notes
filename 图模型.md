我们这周讲的是第8章，图模型。。

---------------------------

我们会从图模型的简介、类型、以及常见任务进行介绍。。。

## 图模型简介

概率图模型是概率论和图结构相结合的产物。

对于传统的机器学习，当我们面临一些问题时，我们希望把这个问题映射到标准的方法上去。

但基于模型的机器学习，我们会思考“什么是适合这个问题的好的模型”，因此，我们会希望，有一个框架可以帮助我们创建新的模型，从而解决我们遇到的问题。。。

概率图模型就是这样的一个框架，，它可以帮助我们设计新的模型，从而对问题进行建模和求解。

--------------------

图主要由节点和边构成。。每个节点表示一个或者一组变量。。边表示这些变量之间的关系。

图模型主要分为两大类：

1. 一类是有向图模型，又叫做贝叶斯网络：边的有向性可以使它表达随机变量之间的因果关系。
2. 另一类是⽆向图模型，又叫做马尔科夫随机场：因为边是无向的，所以它主要表⽰变量之间的关联关系。



## 有向图

刚才说过，图是由边和节点构成的，在有向图中，每条边可以表示成【有序的点对】。

这种父节点和子节点之间通常表示一种因果关系。。比如 患癌症 通常导致 某种血检结果。但在现实世界中，我们遇到的情况通常是观测到血检结果，然后想知道有多大概率是患癌症的。

--------------------

对于一个概率图模型，我们通常关注3个问题，

- 它刻画了这些变量之间什么样的概率分布？如果我们知道了这个概率分布，就可以进行各种边际概率和条件概率的计算。。比如给定一个病人的血检结果，判断他患癌症的概率。

- 这个概率图模型是怎么表达的？是一个什么样的参数化的过程？只有知道它怎么表达的，怎么参数化的，才能把它所表达的概率分布写成式子。

- 这个概率图模型刻画了这些变量之间的哪些条件独立性？ 有了这些条件独立性，就可以组成这个概率图模型的解释。

--------------------

1. 第一个问题：概率分布

​        每个概率图模型是对应着一族概率分布。。图中的每个节点依赖于他的父节点，那每个节点就对应着一个条件概率分布，条件就是该节点的父节点集合。

​        整个概率图的联合概率分布就可以分解为一组局部的条件概率。。每个条件概率就是一个节点依赖于他的父节点集合。。。所谓的一族概率分布，就是说这n个节点的关系只要可以分解成这样的形式，就都可以用这个图模型表示，但具体的概率形式是多样的，可以使高斯分布，泊松分布等等。

​       以这个图为例，它的联合概率分布就可以分解成6部分。。其中x6的父节点有x2和x5两个节点，那它的条件概率分布中的条件就包括这2个节点。

--------------------

2. 第二个问题：这个图模型的具体的表达是什么样的？

​      以这个图为例，假设每个变量是离散的二进制变量。。如果没有这个图模型，我们不知道变量间的局部关系，这6个变量的分布就有2的6次方种可能，，，但通过这个图模型，每一个局部的条件概率可以用2*2的表格或者三维的立方体刻画出来，此时联合概率就可以具体化了。这个时候，描述这6个变量的分布所需要的参数就大大减少了。

--------------------

3. 第三个问题，图模型上所携带的条件独立性有哪些？

   变量之间存在概率上的条件独立性，其实反映到图结构上就对应于某些边不存在。

​       我们可以先看三种最基本的图结构，然后应用到整个图上，就可以判断有向图中存在的所有条件独立性。

- 第一种结构，叫做 【马尔科夫链结构】

​        它存在这样的一种条件独立性，给定中间节点Y，前后两个节点X和Z是独立的。。这也是“马尔科夫假设”，即未来只由当下决定，和过去是没有关系的。

- 第二种结构，叫做【共同的原因】

这种结构中，Y 解释了 X 和Z 之间的依赖，，也就是说，给定节点Y，X节点和Z节点是独立的。

- 第三种结构，叫做【共同效应】

​        当Y未知时，节点X和Z是独立的。。但这种结构很特殊，当Y节点或Y的子节点可观测时，节点X和Z就变得不再独立了，这叫做“解释消除”。

----------------------

​        举个例子，洒水器打开和天下雨都会导致草地是湿的。。假设现在我们观测到草地是湿的，然后判断洒水器打开和天下雨这两个变量是否是独立的，只需要计算这两个条件概率是否相等即可。

​        通过计算，我们可以发现这两个条件概率值是不一样的，也就是说，知道草地是湿的，那么下雨和洒水器打开变成互相依赖的了。

​     【这几个推导都是很简单的，，我就不展开公式的具体计算了，，有兴趣的话可以具体看ppt】

​       有了这三种最基本结构的条件独立性之后，我们就可以判断任意图中存在的条件独立性了。

----------------------

​        总体而言，一个有向图模型是和一族概率分布相关联的。，对于一个有向图模型，我们有两种等价的方式来描述它，可以用一组分解后的局部条件概率的乘积表示，也可以用一组条件独立陈述进行描述。



## 无向图

无向图，和有向图类似，只是边是无向的，不再表示因果关系，只能是说一对节点之间有关联关系。

我们同样关心这3个问题，一个图对应什么样的概率分布？应该怎么参数化？包含着哪些条件独立性？

----------------------

3. 我们首先看条件独立性：

在无向图中，条件独立性用简单的图分割理论就可以进行判断。。比如左边这个图，给定图示的两个观测变量，它可以把 X_A集合 和 X_c集合 中的节点完全隔开，则 给定X_B， X_A和X_c 这样的条件独立性存在。

在无向图模型中，还有一个“马尔科夫毯”的概念。一个节点的所有邻居节点构成了该节点的马尔科夫毯。。所以，对于任何节点，只要给定了他的马尔科夫毯，他就跟外界隔绝了，也就是和外界所有其他的变量条件独立了。

----------------------

1.  我们再看第一个问题，一个无向图模型的概率分布是什么样的？

​        对于有向图，我们可以用局部的条件概率分布定义全局的联合概率。。但这对于无向图模型不再成立。因为它不再有局部的因果关系这样的性质。。在这样的情况下，我们仍然希望能够找到一种方式，它有足够的能力定义不同的函数，而且保持局部函数乘积的形式。

----------------------

​        首先，关键问题是我们需要确定局部函数的定义域是什么。。这个定义域的确定可以借助我们之前说的条件独立性。。。如果一个节点集合，他可以被图分割给分割开来，那么局部函数就不应该定义在这个节点集合上。。   

​        到现在，我们可以想到图上的一个概念——团。图上的团是一个全连接的节点集合，因此可以保证它不可能被分割开来，所以局部函数应该定义在团上。。不失一般性，我们可以在极大团上定义局部函数。。这样的局部函数，我们通常称为势函数，它是一个严格的正的实值函数。

----------------------

​        以这个图为例，它包含5个极大团，得到定义域之后，我们就可以把这个无向图的联合概率分布表示为定义在这些极大团上的势函数的乘积，再除以一个正则化因子Z。。通过除以Z可以保证概率归一化。

----------------------

2. 表示

​        类似于有向图，我们以这个为例，假设每个变量是离散的二进制变量。。通过在每个极大团上定义势函数，可以把变量分布的可能性从 2的n次方 减少成 r*2^k。

----------------------

​        关于势函数，通常它既不是条件概率也不是边际概率。。但他也有自己的意义，从物理的角度来讲，势函数它通常定义的是，一个团上各个变量之间的一致性，或者他们的约束、能量等。

​        我们可以把无向图模型的联合概率表示成一个更一般的形式，因为我们有要求势函数必须是严格的正实值函数，所以我们用指数的形式。。这就是玻尔兹曼分布。

----------------------

​        总结而言，一个无向图所表示的一族概率分布，我们有两种方式来定义它，一种是枚举所有可能的势函数的形式，，另一种是列出这个图上所有的条件独立性。。。这两种方式定义出来的图的分布是相同的。



## 推断

我们有了图的概率分布表示之后，就可以做一些推断的任务。。即知道图中的部分观测变量后，计算一个或多个其他节点的后验概率分布。。。

还有一个典型的任务是，通过观测数据来学习出模型的参数。。但对于贝叶斯学派而言，这也是一个推断任务，因为在他们的理论中，参数也视为变量，可以通过最大后验概率来推断出模型的参数取值。

推断算法一般分为精确推断和近似推断。。精确推断包括变量消去法、信念传播算法等，一般会有较高的计算代价。在一些精确推断不好求解的情况下，也会用采样、变分推断等近似方法。。。这里，我们只讲精确推断的方法。

----------------------

1. 变量消去法

以这个有向图模型为例，假设我们要计算 X5节点 的边缘概率。我们可以对这个图模型所表示的联合概率分布在x1到x4节点上进行积分，从而消去x1到x4节点。。利用联合概率分布中的条件独立性，我们可以把要积分掉的变量推到只和它有关的部分，然后逐个消去。

----------------------

我们可以把这个过程写成一个更形式化地，可迭代的计算过程。变量消去过程可以看做是在图上传递一些消息。第一步是从x1传递过来的消息，对x1求和，就是把x1的消息传递给x2，，接着对x2求和，就是把消息从x2传递到x3，，然后对x4求和，把x4的消息传递到x3，，最后对x3求和，就是把x3从两边收集到的消息传递给x5。

这个过程就可以形式化成下面这个公式，就是【加和-乘积算法】。节点i到节点j传递消息，就是节点i先收集到它的邻居传递给他的消息，然后再根据节点i和节点j之间的关系，将节点i的消息传递给节点j。

通过这个重复的【加和-乘积】计算，我们就可以得到一个节点的边际概率。这个算法也叫做信念传播法。

----------------------

2. 通过信念传播算法，我们可以定义一个更系统化的方法来进行这个求解过程。

假设我们现在要计算任何一个节点的边缘概率，我们不需要对每个节点分别进行多次迭代来计算。。只需要进行两轮完整的传播，就可以得到任何节点的边缘概率。

我们的做法是，首先先指定一个节点作为根节点，以这个图为例，x1节点为根节点。。首先从叶子节点开始传递消息，直到根节点接收到所有邻居节点传来的信息。。然后从根节点开始传播信息，直到所有叶子节点接收到信息。。。至此，所有节点都接收到了所有方向的邻居传来的消息。

----------------------

我们可以举个例子，假设有一个无向图模型，它的联合概率分布可以表示成这个样子。。我们以x1节点为根节点，先从叶子节点到根节点进行一次消息传递，，然后再从根节点到叶子节点进行一次消息传递。。至此，我们就可以得到任何一个节点的边际分布了。

----------------------

3. 最大加和算法

除了求解边际概率，还有两个常见的任务是，找到使得观测变量具有最大概率的一个设置，以及这个概率值。

最大加和算法可以用来求解这两个问题。

求解步骤是，指派一个节点为根节点，从叶子节点开始传递消息，直到根节点接收到所有邻居节点传来的消息，最后对到达根节点的信息的乘积进行最大化，得到最大概率值。这叫做 “最大化乘积算法”。。但在实际应用中，为了防止许多小概率相乘造成的数值下溢问题，通常的做法是对联合概率分布取对数，从而变成了对数的加和操作，即“最大化加和算法”。。



## 总结

最后我们来总结一下：

  1. 图模型是一个强有力的工具，可以帮助我们对问题进行建模和求解。

2. 它主要分为有向图模型和无向图模型两类。

3. 给定一个图模型，我们可以使用信念传播算法计算节点的边缘概率，，也可以使用最大加和算法，通过最大化观测变量的后验概率来进行参数求解。